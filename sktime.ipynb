{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [22509360, 10421]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m Y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mStepOne_LabelTrain.csv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39miloc[:,\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m     36\u001b[0m knn \u001b[39m=\u001b[39m KNeighborsTimeSeriesClassifier(n_neighbors\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, Y)\n\u001b[1;32m     39\u001b[0m knn\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     40\u001b[0m y_pred \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m   2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [22509360, 10421]"
     ]
    }
   ],
   "source": [
    "# # from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "\n",
    "# # # load your 20 columns pandas dataset as X_train and your 5 columns pandas dataset as y_train\n",
    "# # X_train = pd.read_csv(\"InputTrain.csv\").iloc[:,2:]\n",
    "# # # reshape X_train to be a list of time series data\n",
    "# # X_train = [X_train.iloc[:, i].values.reshape(-1, 1) for i in range(X_train.shape[1])]\n",
    "# # X_train = np.concatenate(X_train, axis=1)\n",
    "\n",
    "# # # convert y_train to a numpy array\n",
    "# # y_train = pd.read_csv('StepOne_LabelTrain.csv').iloc[:,2]\n",
    "# # y_train = y_train.values\n",
    "# # # create the KNN classifier with k=1\n",
    "# # knn = KNeighborsTimeSeriesClassifier(n_neighbors=1)\n",
    "\n",
    "# # # fit the classifier to the training data\n",
    "# # knn.fit(X_train, y_train)\n",
    "\n",
    "# # # make predictions on new data\n",
    "# # # reshape your new data (X_test) in the same way as X_train\n",
    "# # X_test = pd.read_csv(\"InputTest.csv\").iloc[:,2:]\n",
    "# # X_test = [X_test.iloc[:, i].values.reshape(-1, 1) for i in range(X_test.shape[1])]\n",
    "# # X_test = np.concatenate(X_test, axis=1)\n",
    "# # y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "# dfX = pd.read_csv(\"InputTrain.csv\").iloc[:,2:]\n",
    "# # melt the dataframe to create a new dataframe with one column\n",
    "# new_df = pd.melt(dfX)\n",
    "# X = new_df[\"value\"].values\n",
    "# Y = pd.read_csv(\"StepOne_LabelTrain.csv\").iloc[:,2].to_numpy()\n",
    "# knn = KNeighborsTimeSeriesClassifier(n_neighbors=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "# knn.fit(X_train, y_train)\n",
    "# y_pred = knn.predict(X_test)\n",
    "# accuracy = knn.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "# transformers in scikit-learn maybe the solution to this \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "# transformers in scikit-learn maybe the solution to this \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "# transformers in scikit-learn maybe the solution to this \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2488, 2160)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import re\n",
    "import pywt\n",
    "\n",
    "X = pd.read_csv(\"InputTrain.csv\").iloc[:,2:]\n",
    "y = pd.read_csv(\"StepOne_LabelTrain.csv\").iloc[:,2:]\n",
    "dfXTest = pd.read_csv(\"InputTest.csv\").iloc[:,2:]\n",
    "#l'ajout manuel des features prend un peu de temps c'est pas opti, ~50sec pour la cellule enti√®re\n",
    "\n",
    "# Ajout manuel de 3 features (qui utilisent des Series et pas le dataframe entier)\n",
    "autocor = []\n",
    "idmax = []\n",
    "entropy = []\n",
    "for i in range(0,10421) :\n",
    "  autocor.append(X.iloc[i,0:2161].autocorr(216))\n",
    "  idmax.append((int)(re.search(r'\\d+',X.iloc[i,0:2161].idxmax())[0]))\n",
    "  entropy.append(sum([ p * math.log(p) / math.log(2.0) for p in X.iloc[i,0:2161]]))\n",
    "# X['autocor'] = autocor\n",
    "X['idmax'] = idmax\n",
    "X['entropy'] = entropy\n",
    "autocor = []\n",
    "idmax = []\n",
    "entropy = []\n",
    "print(dfXTest.shape)\n",
    "for i in range(0,2488) :\n",
    "  autocor.append(dfXTest.iloc[i,0:2161].autocorr(216))\n",
    "  idmax.append((int)(re.search(r'\\d+',dfXTest.iloc[i,0:2161].idxmax())[0]))\n",
    "  entropy.append(sum([ p * math.log(p) / math.log(2.0) for p in dfXTest.iloc[i,0:2161]]))\n",
    "# dfXTest['autocor'] = autocor\n",
    "dfXTest['idmax'] = idmax\n",
    "dfXTest['entropy'] = entropy\n",
    "\n",
    "\n",
    "# Ajout automatique de 6 features (qui utilisent le dataframe entier)\n",
    "X['mean'] = X.iloc[:, 2:2161].mean(axis=1)\n",
    "\n",
    "X['median'] = X.iloc[:, 2:2161].median(axis=1)\n",
    "\n",
    "X['std'] = X.iloc[:, 2:2161].std(axis=1)\n",
    "\n",
    "X['var'] = X.iloc[:, 2:2161].var(axis=1)\n",
    "\n",
    "X['skew'] = X.iloc[:, 2:2161].skew(axis=1)\n",
    "\n",
    "X['sem'] = X.iloc[:, 2:2161].sem(axis=1)\n",
    "\n",
    "\n",
    "dfXTest['mean'] = dfXTest.iloc[:, 2:2161].mean(axis=1)\n",
    "\n",
    "dfXTest['median'] = dfXTest.iloc[:, 2:2161].median(axis=1)\n",
    "\n",
    "dfXTest['std'] = dfXTest.iloc[:, 2:2161].std(axis=1)\n",
    "\n",
    "dfXTest['var'] = dfXTest.iloc[:, 2:2161].var(axis=1)\n",
    "\n",
    "dfXTest['skew'] = dfXTest.iloc[:, 2:2161].skew(axis=1)\n",
    "\n",
    "dfXTest['sem'] = dfXTest.iloc[:, 2:2161].sem(axis=1)\n",
    "\n",
    "\n",
    "def wavelet_variance(row):\n",
    "    coeffs = pywt.wavedec(row, 'db4', level=5) # perform 5-level decomposition using db4 wavelet\n",
    "    variances = [c.var() for c in coeffs] # calculate the variances of each detail coefficient\n",
    "    \n",
    "    return sum(variances) # return the sum of variances\n",
    "\n",
    "X['WaveletVar'] = X.apply(wavelet_variance, axis=1)\n",
    "dfXTest['WaveletVar'] = dfXTest.apply(wavelet_variance, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# scaled_x = scaler.fit_transform(X)\n",
    "# scaled_T = scaler.fit_transform(dfXTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.42286\teval-rmse:0.43974\n",
      "[1]\ttrain-rmse:0.36280\teval-rmse:0.39682\n",
      "[2]\ttrain-rmse:0.31510\teval-rmse:0.36568\n",
      "[3]\ttrain-rmse:0.27602\teval-rmse:0.34432\n",
      "[4]\ttrain-rmse:0.24436\teval-rmse:0.32921\n",
      "[5]\ttrain-rmse:0.21739\teval-rmse:0.31854\n",
      "[6]\ttrain-rmse:0.19466\teval-rmse:0.31107\n",
      "[7]\ttrain-rmse:0.17602\teval-rmse:0.30569\n",
      "[8]\ttrain-rmse:0.16061\teval-rmse:0.30180\n",
      "[9]\ttrain-rmse:0.14689\teval-rmse:0.29906\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train,y_train)\n",
    "dtest = xgb.DMatrix(X_test,y_test)\n",
    "\n",
    "model = xgb.XGBModel(n_estimators=10)\n",
    "params = {\n",
    "    'max_depth': 20,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'booster':'gbtree',\n",
    "    'subsample': 0.85,\n",
    "    'learning_rate':0.2,\n",
    "\n",
    "}\n",
    "\n",
    "# Train model\n",
    "evalist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "model = xgb.train(params, dtrain,evals=evalist)\n",
    "\n",
    "dFin = xgb.DMatrix(dfXTest)\n",
    "\n",
    "preds = model.predict(dFin) #Predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfF = pd.DataFrame(preds, columns=[\"Washing Machine\",\"Dishwasher\",\"Tumble Dryer\",\"Microwave\",\"Kettle\"])\n",
    "dfF = dfF.rename_axis('Index')\n",
    "dfF.to_csv(\"oneT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8317920184190329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# totF =0\n",
    "# for k in range (1,5) :\n",
    "#     dfX['std'] = dfX.iloc[:, 2:].std(axis=1)\n",
    "#     dfX['var'] = dfX.iloc[:, 2:].var(axis=1)\n",
    "#     total = 0\n",
    "\n",
    "#     for i in range(2,7):\n",
    "#         dfY = pd.read_csv(\"StepOne_LabelTrain.csv\").iloc[:,i].to_numpy()\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(dfX, dfY)\n",
    "#         classifier = DummyClassifier(strategy=\"prior\")\n",
    "#         classifier.fit(X_train, y_train)\n",
    "#         total += classifier.score(X_test, y_test)\n",
    "\n",
    "#     totF += total /5\n",
    "# print(totF/4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Washing Machine</th>\n",
       "      <th>Dishwasher</th>\n",
       "      <th>Tumble Dryer</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>Kettle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.196328</td>\n",
       "      <td>0.155483</td>\n",
       "      <td>0.072301</td>\n",
       "      <td>0.170945</td>\n",
       "      <td>0.278544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>0.106424</td>\n",
       "      <td>0.365963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052177</td>\n",
       "      <td>0.043402</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.056172</td>\n",
       "      <td>0.227191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.136407</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>0.139150</td>\n",
       "      <td>0.290110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.228074</td>\n",
       "      <td>0.159977</td>\n",
       "      <td>0.176064</td>\n",
       "      <td>0.239632</td>\n",
       "      <td>0.400129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>0.266665</td>\n",
       "      <td>0.185597</td>\n",
       "      <td>0.015363</td>\n",
       "      <td>0.181303</td>\n",
       "      <td>0.251275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>0.192667</td>\n",
       "      <td>0.160788</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.280159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>0.325879</td>\n",
       "      <td>0.309132</td>\n",
       "      <td>0.199683</td>\n",
       "      <td>0.301394</td>\n",
       "      <td>0.374529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>0.250574</td>\n",
       "      <td>0.226559</td>\n",
       "      <td>0.093387</td>\n",
       "      <td>0.164121</td>\n",
       "      <td>0.278832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>0.218751</td>\n",
       "      <td>0.210953</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.183130</td>\n",
       "      <td>0.279202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Washing Machine  Dishwasher  Tumble Dryer  Microwave    Kettle\n",
       "0            0.196328    0.155483      0.072301   0.170945  0.278544\n",
       "1            0.158500    0.265667      0.021269   0.106424  0.365963\n",
       "2            0.052177    0.043402      0.006387   0.056172  0.227191\n",
       "3            0.161630    0.136407      0.040873   0.139150  0.290110\n",
       "4            0.228074    0.159977      0.176064   0.239632  0.400129\n",
       "...               ...         ...           ...        ...       ...\n",
       "2080         0.266665    0.185597      0.015363   0.181303  0.251275\n",
       "2081         0.192667    0.160788      0.074388   0.180723  0.280159\n",
       "2082         0.325879    0.309132      0.199683   0.301394  0.374529\n",
       "2083         0.250574    0.226559      0.093387   0.164121  0.278832\n",
       "2084         0.218751    0.210953      0.071784   0.183130  0.279202\n",
       "\n",
       "[2085 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(preds,columns=[\"Washing Machine\",\"Dishwasher\",\"Tumble Dryer\",\"Microwave\",\"Kettle\"])\n",
    "# y_test\n",
    "# # acc_rfc = (df == y_test).sum().astype(float) / len(preds)*100\n",
    "\n",
    "# # df.to_csv(\"XGB1.csv\")\n",
    "# # y_test\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Washing Machine</th>\n",
       "      <th>Dishwasher</th>\n",
       "      <th>Tumble Dryer</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>Kettle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8233</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6246</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10409</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Washing Machine  Dishwasher  Tumble Dryer  Microwave  Kettle\n",
       "8233                 0           0             0          0       0\n",
       "5860                 0           0             0          0       0\n",
       "3463                 0           0             0          0       0\n",
       "3911                 0           0             0          0       1\n",
       "6246                 0           0             0          0       1\n",
       "...                ...         ...           ...        ...     ...\n",
       "7035                 0           0             0          0       0\n",
       "4207                 0           0             0          0       0\n",
       "6663                 0           0             0          0       0\n",
       "10409                1           0             0          0       0\n",
       "5878                 0           0             0          0       0\n",
       "\n",
       "[2085 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df.to_csv(\"XGB1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# create a DMATRIX from the training data\n",
    "\n",
    "# create an XGBoost model\n",
    "xgb_model = xgb.XGBModel()\n",
    "\n",
    "# train the model on the DMATRIX\n",
    "xgb_model.fit(dtrain)\n",
    "\n",
    "# use the model to make predictions on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred\n",
    "preds\n",
    "df = pd.DataFrame(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
