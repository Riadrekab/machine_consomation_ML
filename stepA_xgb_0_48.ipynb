{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVaM-Zdj7qcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8b0dcd-7898-4c97-c769-cc0b3476dae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive#On monte le drive pour charger les datasets depuis google drive (optionnel)\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "%cd gdrive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nx8L6p017qcZ",
        "outputId": "a3e80a17-1670-4712-97a2-9334a4601b84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef wavelet_variance(row):\\n    coeffs = pywt.wavedec(row, 'db4', level=5) # perform 5-level decomposition using db4 wavelet\\n    variances = [c.var() for c in coeffs] # calculate the variances of each detail coefficient\\n    \\n    return sum(variances) # return the sum of variances\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pywt\n",
        "\n",
        "def calculate_features(df): #fonction de feature engineering appliquée sur le dataset d'entrainement et le dataset de test\n",
        "  # Ajout manuel de features\n",
        "  #autocor = []\n",
        "  #idmax = []\n",
        "  entropy = []\n",
        "  #df['WaveletVar'] = df.apply(wavelet_variance, axis=1)\n",
        "  for i in range(len(df.index)) :\n",
        "    #autocor.append(df.iloc[i,0:2160].autocorr(1))\n",
        "    #idmax.append((int)(re.search(r'\\d+',df.iloc[i,0:2160].idxmax())[0]))\n",
        "    entropy.append(sum([ p * math.log(p) / math.log(2.0) for p in df.iloc[i,0:2160]]))\n",
        "  #df['autocor'] = autocor\n",
        "  #df['idmax'] = idmax\n",
        "  df['entropy'] = entropy\n",
        "\n",
        "  # Ajout automatique de features\n",
        "  df['mean'] = df.iloc[:, 0:2160].mean(axis=1)\n",
        "  df['min'] = df.iloc[:,0:2160].min(axis=1)\n",
        "  df['max'] = df.iloc[:,0:2160].max(axis=1)\n",
        "  df['median'] = df.iloc[:, 0:2160].median(axis=1)\n",
        "  df['std'] = df.iloc[:, 0:2160].std(axis=1)\n",
        "  df['var'] = df.iloc[:, 0:2160].var(axis=1)\n",
        "  df['skew'] = df.iloc[:, 0:2160].skew(axis=1)\n",
        "  df['sem'] = df.iloc[:, 0:2160].sem(axis=1)\n",
        "\n",
        "X = pd.read_csv(\"InputTrain.csv\").iloc[:,2:]\n",
        "y = pd.read_csv(\"StepOne_LabelTrain.csv\").iloc[:,2:]\n",
        "dfXTest = pd.read_csv(\"InputTest.csv\").iloc[:,2:]\n",
        "\n",
        "calculate_features(X)\n",
        "calculate_features(dfXTest)\n",
        "'''\n",
        "def wavelet_variance(row):\n",
        "    coeffs = pywt.wavedec(row, 'db4', level=5) # perform 5-level decomposition using db4 wavelet\n",
        "    variances = [c.var() for c in coeffs] # calculate the variances of each detail coefficient\n",
        "    \n",
        "    return sum(variances) # return the sum of variances\n",
        "'''\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.isna().sum().sum()\n",
        "#On vérifie qu'il n'y a aucune valeur NaN dans le dataset (peut causer beaucoup de problèmes à l'entrainement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVwf-Q2rhMMH",
        "outputId": "4d438508-0db7-4dfa-c8d2-048e3d574c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLbGSjht7qcd"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4,random_state=42)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train,label=y_train)\n",
        "dtest = xgb.DMatrix(X_test,label=y_test)\n",
        "\n",
        "model = xgb.XGBModel()\n",
        "params = {\n",
        "    'max_depth': 10,\n",
        "    'eta': 0.1,\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'error',\n",
        "    'booster':'dart',\n",
        "    'sample_type':'weighted',\n",
        "    'normalize_type':'forest',\n",
        "    'learning_rate':0.4,\n",
        "    'tree_method':'gpu_hist',\n",
        "    'n_jobs':500,\n",
        "    'rate_drop':0.3,\n",
        "    'skip_drop':0.2,\n",
        "\n",
        "}\n",
        "\n",
        "# Train model\n",
        "evalist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "model = xgb.train(params, dtrain,num_boost_round=200)\n",
        "\n",
        "preds = model.predict(dtest) #Predictions\n",
        "y_test = y_test.astype(float)\n",
        "print(\"Precision = {}\".format(average_precision_score(y_test.values, preds, average='macro')))\n",
        "print(\"Precision = {}\".format(average_precision_score(y_test.values, preds, average='weighted')))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8kYgcyI7qce"
      },
      "outputs": [],
      "source": [
        "dFin = xgb.DMatrix(dfXTest)\n",
        "preds_f = model.predict(dFin)\n",
        "\n",
        "dfF = pd.DataFrame(preds_f, columns=[\"Washing Machine\",\"Dishwasher\",\"Tumble Dryer\",\"Microwave\",\"Kettle\"])\n",
        "dfF = dfF.rename_axis('Index')\n",
        "dfF.to_csv(\"corr3.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}