{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "<h2 style=\"color:red\"> Transposing the datasets </h2>\n",
    "\n",
    "As described in the report, we operate a specific transposition to obtain a better shape of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(\"InputTrain.csv\").iloc[:,2:]\n",
    "dfXTest = pd.read_csv(\"InputTest.csv\").iloc[:,2:]\n",
    "Y1 = pd.read_csv(\"StepTwo_LabelTrain_WashingMachine.csv\").iloc[:,2:]\n",
    "Y2 = pd.read_csv(\"StepTwo_LabelTrain_Dishwasher.csv\").iloc[:,2:]\n",
    "Y3 = pd.read_csv(\"StepTwo_LabelTrain_TumbleDryer.csv\").iloc[:,2:]\n",
    "Y4 = pd.read_csv(\"StepTwo_LabelTrain_Microwave.csv\").iloc[:,2:]\n",
    "Y5 = pd.read_csv(\"StepTwo_LabelTrain_Kettle.csv\").iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "consumption_values = dfXTest.values\n",
    "consumption_values = consumption_values.reshape(consumption_values.shape[0], -1)\n",
    "\n",
    "# Remove the mean and scale the values to have unit variance\n",
    "scaler = StandardScaler()\n",
    "normalized_consumption_values = scaler.fit_transform(consumption_values)\n",
    "dfXTest.iloc[:, 0:] = normalized_consumption_values.reshape(normalized_consumption_values.shape[0], -1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "consumption_values = X.values\n",
    "consumption_values = consumption_values.reshape(consumption_values.shape[0], -1)\n",
    "\n",
    "# Remove the mean and scale the values to have unit variance\n",
    "scaler = StandardScaler()\n",
    "normalized_consumption_values = scaler.fit_transform(consumption_values)\n",
    "X.iloc[:, 0:] = normalized_consumption_values.reshape(normalized_consumption_values.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(Y1)):\n",
    "    y1 = pd.concat([y1, pd.DataFrame(Y1.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "y2 = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(Y1)):\n",
    "    y2 = pd.concat([y2, pd.DataFrame(Y2.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "y3 = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(Y3)):\n",
    "    y3 = pd.concat([y3, pd.DataFrame(Y3.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "\n",
    "y4 = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(Y4)):\n",
    "    y4 = pd.concat([y4, pd.DataFrame(Y4.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "y5 = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(Y5)):\n",
    "    y5 = pd.concat([y5, pd.DataFrame(Y5.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "x = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(X)):\n",
    "    x = pd.concat([x, pd.DataFrame(X.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "\n",
    "\n",
    "dfTest = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row of the original dataframe and append the transposed row to the new dataframe\n",
    "for i in range(len(dfXTest)):\n",
    "    dfTest = pd.concat([dfTest, pd.DataFrame(dfXTest.iloc[i].values.reshape(1, -1).T)])\n",
    "\n",
    "x.to_csv(\"x_transposed_notNormalized.csv\")\n",
    "dfTest.to_csv(\"dfTest_transposed_notNormalized.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "<h2 style=\"color:red\"> Adding the features  </h2>\n",
    "\n",
    "In the following part, we will add the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x = pd.read_csv(\"../x_transposed_notNormalized.csv\")\n",
    "dfxTest= pd.read_csv(\"../dfTest_transposed_notNormalized.csv\")\n",
    "\n",
    "listI = [10,20,50,60,65,70,80,90,100,120,110]\n",
    "for i in listI:\n",
    "    x[f'consumption_lag_{i}'] = x['0'].shift(i)\n",
    "    dfxTest[f'consumption_lag_{i}'] = dfxTest['0'].shift(i)\n",
    "\n",
    "\n",
    "# replace NaN values with previous non-NaN values\n",
    "x = x.fillna(method='bfill')\n",
    "dfxTest = dfxTest.fillna(method='bfill')\n",
    "x = x.drop('Unnamed: 0',axis = 1)\n",
    "\n",
    "dfxTest = dfxTest.drop('Unnamed: 0',axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['rolling_skewness20'] = x['0'].rolling(20).skew()\n",
    "dfxTest['rolling_skewness20'] = dfxTest['0'].rolling(20).skew()\n",
    "x['rolling_kurtosis20'] = x['0'].rolling(20).kurt()\n",
    "dfxTest['rolling_kurtosis20'] = dfxTest['0'].rolling(20).kurt() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['rolling_std2'] = x['rolling_std2'].interpolate(method='linear', limit_direction='both')\n",
    "x['rolling_std3'] = x['rolling_std3'].interpolate(method='linear', limit_direction='both')\n",
    "dfxTest['rolling_std2'] = dfxTest['rolling_std2'].interpolate(method='linear', limit_direction='both')\n",
    "dfxTest['rolling_std3'] = dfxTest['rolling_std3'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "x['rolling_skewness20'] = x['rolling_skewness20'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "x['rolling_kurtosis20'] = x['rolling_kurtosis20'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "dfxTest['rolling_skewness20'] = dfxTest['rolling_skewness20'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "dfxTest['rolling_kurtosis20'] = dfxTest['rolling_kurtosis20'].interpolate(method='linear', limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelmax, argrelmin\n",
    "\n",
    "def time_since_last_extreme(data, find_extreme_func):\n",
    "    extreme_indices, = find_extreme_func(data)  # Unpack the tuple\n",
    "    time_since_last_extreme = np.zeros_like(data)\n",
    "    for idx in range(1, len(extreme_indices)):\n",
    "        time_since_last_extreme[extreme_indices[idx-1]:extreme_indices[idx]] = np.arange(extreme_indices[idx] - extreme_indices[idx-1])\n",
    "    time_since_last_extreme[extreme_indices[-1]:] = np.arange(len(data) - extreme_indices[-1])\n",
    "    return time_since_last_extreme\n",
    "\n",
    "x['time_since_last_max'] = time_since_last_extreme(x['0'].values, argrelmax)\n",
    "x['time_since_last_min'] = time_since_last_extreme(x['0'].values, argrelmin)\n",
    "dfxTest['time_since_last_max'] = time_since_last_extreme(dfxTest['0'].values, argrelmax)\n",
    "dfxTest['time_since_last_min'] = time_since_last_extreme(dfxTest['0'].values, argrelmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('../x-official_df.csv')\n",
    "dfxTest.to_csv('../test-official_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "<h2 style=\"color:red\"> Predictions  </h2>\n",
    "\n",
    "In the final part, we will train,test the model, and then run the model on the final dataset to export to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.read_csv(\"../x-official_df.csv\")\n",
    "x = x.drop(\"Unnamed: 0\",axis=1)\n",
    "y = pd.read_csv(\"../y_ready2.csv\")\n",
    "y = y.drop(\"Unnamed: 0\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3,random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3,shuffle=False)\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train,label=y_train)\n",
    "dtest = xgb.DMatrix(X_test,label=y_test)\n",
    "\n",
    "model = xgb.XGBModel(n_estimators=150)\n",
    "params = {\n",
    "    'max_depth': 10,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'map',\n",
    "    'booster':'gbtree',\n",
    "    'normalize_type':'forest',\n",
    "    'learning_rate':0.4,\n",
    "    'tree_method':'gpu_hist',\n",
    "    'n_jobs':500,\n",
    "    'colsample_bytree':0.8,\n",
    "    'gamma':0.4,\n",
    "    'subsample':0.8\n",
    "}\n",
    "\n",
    "# Train model\n",
    "evalist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "model = xgb.train(params, dtrain,num_boost_round=60,verbose_eval=1,evals=evalist)\n",
    "\n",
    "preds = model.predict(dtest) #Predictions\n",
    "y_test = y_test.astype(float)\n",
    "optimal_num_boost_round = model.best_iteration\n",
    "\n",
    "# print(\"Precision = {}\".format(average_precision_score(y_test.values, preds, average='macro')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxTest= pd.read_csv(\"../test-FInalMoreLag.csv\")\n",
    "\n",
    "dfxTest = dfxTest.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "dFin = xgb.DMatrix(dfxTest)\n",
    "\n",
    "preds_f = model.predict(dFin)\n",
    "\n",
    "dfF = pd.DataFrame(preds_f, columns=[\"Washing Machine\",\"Dishwasher\",\"Tumble Dryer\",\"Microwave\",\"Kettle\"])\n",
    "dfF = dfF.rename_axis('Index')\n",
    "dfF = dfF.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfF.to_csv(\"../Final.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
