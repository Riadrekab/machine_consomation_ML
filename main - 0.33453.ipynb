{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LSc3HtRnJ7w"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "This ipynb file is used to build a model for a time series classification problem. \n",
        "<br>\n",
        "\n",
        "We will start by : \n",
        "<br>\n",
        "1- Importing the training datasets.\n",
        "<br>\n",
        "2- Merge the two datasets \n",
        "<br>\n",
        "3- Add new features (Mean, median, etc...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "xClQmgjWnJ7z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dfX= pd.read_csv(\"InputTrain.csv\")\n",
        "dfY = pd.read_csv(\"StepOne_LabelTrain.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "#l'ajout manuel des features prend un peu de temps c'est pas opti, ~50sec pour la cellule entière\n",
        "\n",
        "# Ajout manuel de 3 features (qui utilisent des Series et pas le dataframe entier)\n",
        "autocor = []\n",
        "idmax = []\n",
        "entropy = []\n",
        "for i in range(0,10421) :\n",
        "  autocor.append(dfX.iloc[i,2:2161].autocorr(216))\n",
        "  idmax.append((int)(re.search(r'\\d+',dfX.iloc[i,2:2161].idxmax())[0]))\n",
        "  entropy.append(sum([ p * math.log(p) / math.log(2.0) for p in dfX.iloc[i,2:2161]]))\n",
        "dfX['autocor'] = autocor\n",
        "dfX['idmax'] = idmax\n",
        "dfX['entropy'] = entropy\n",
        "\n",
        "# Ajout automatique de 6 features (qui utilisent le dataframe entier)\n",
        "dfX['mean'] = dfX.iloc[:, 2:2161].mean(axis=1)\n",
        "\n",
        "dfX['median'] = dfX.iloc[:, 2:2161].median(axis=1)\n",
        "\n",
        "dfX['std'] = dfX.iloc[:, 2:2161].std(axis=1)\n",
        "\n",
        "dfX['var'] = dfX.iloc[:, 2:2161].var(axis=1)\n",
        "\n",
        "dfX['skew'] = dfX.iloc[:, 2:2161].skew(axis=1)\n",
        "\n",
        "dfX['sem'] = dfX.iloc[:, 2:2161].sem(axis=1)\n",
        "\n",
        "dfX.fillna(0,inplace=True)"
      ],
      "metadata": {
        "id": "mkPSdh-hIASu"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
        "\n",
        "# Importer les données d'entrainement\n",
        "train_input = dfX.drop([\"Index\"],axis=1)\n",
        "train_labels = pd.read_csv('StepOne_LabelTrain.csv').iloc[:, 2:7]\n",
        "\n",
        "# Normaliser les données d'entrée\n",
        "scaler = MinMaxScaler()\n",
        "train_input_scaled = scaler.fit_transform(train_input.iloc[:, 2:])\n",
        "train_input = np.expand_dims(train_input_scaled, axis=2)\n",
        "\n",
        "# Créer les 5 modèles de classification binaire pour chaque appareil\n",
        "models = []\n",
        "for i in range(5):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(train_input.shape[1], 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(LSTM(32, return_sequences=True))\n",
        "    model.add(LSTM(16, return_sequences=True))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile('adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_input, train_labels.iloc[:, i], epochs=5, batch_size=64, validation_split=0.2)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O8gfGgsZxGl",
        "outputId": "73539040-2892-4948-89f7-d28737d7d496"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "131/131 [==============================] - 34s 67ms/step - loss: 0.4344 - accuracy: 0.7988 - val_loss: 0.4332 - val_accuracy: 0.8082\n",
            "Epoch 2/5\n",
            "131/131 [==============================] - 7s 54ms/step - loss: 0.3931 - accuracy: 0.8205 - val_loss: 0.4330 - val_accuracy: 0.8091\n",
            "Epoch 3/5\n",
            "131/131 [==============================] - 8s 60ms/step - loss: 0.3732 - accuracy: 0.8252 - val_loss: 0.4391 - val_accuracy: 0.7976\n",
            "Epoch 4/5\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 0.3518 - accuracy: 0.8385 - val_loss: 0.4480 - val_accuracy: 0.7789\n",
            "Epoch 5/5\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 0.3269 - accuracy: 0.8521 - val_loss: 0.4332 - val_accuracy: 0.7909\n",
            "Epoch 1/5\n",
            "131/131 [==============================] - 13s 61ms/step - loss: 0.3389 - accuracy: 0.8436 - val_loss: 0.2725 - val_accuracy: 0.8825\n",
            "Epoch 2/5\n",
            "131/131 [==============================] - 8s 61ms/step - loss: 0.2769 - accuracy: 0.8678 - val_loss: 0.2698 - val_accuracy: 0.8710\n",
            "Epoch 3/5\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 0.2313 - accuracy: 0.8994 - val_loss: 0.2922 - val_accuracy: 0.8681\n",
            "Epoch 4/5\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 0.1968 - accuracy: 0.9181 - val_loss: 0.3353 - val_accuracy: 0.8513\n",
            "Epoch 5/5\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 0.1720 - accuracy: 0.9316 - val_loss: 0.3332 - val_accuracy: 0.8619\n",
            "Epoch 1/5\n",
            "131/131 [==============================] - 13s 67ms/step - loss: 0.1892 - accuracy: 0.9427 - val_loss: 0.0361 - val_accuracy: 0.9952\n",
            "Epoch 2/5\n",
            "131/131 [==============================] - 8s 60ms/step - loss: 0.1483 - accuracy: 0.9556 - val_loss: 0.0886 - val_accuracy: 0.9899\n",
            "Epoch 3/5\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 0.1352 - accuracy: 0.9598 - val_loss: 0.0336 - val_accuracy: 0.9966\n",
            "Epoch 4/5\n",
            "131/131 [==============================] - 8s 61ms/step - loss: 0.1261 - accuracy: 0.9633 - val_loss: 0.0815 - val_accuracy: 0.9832\n",
            "Epoch 5/5\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 0.1206 - accuracy: 0.9653 - val_loss: 0.0408 - val_accuracy: 0.9914\n",
            "Epoch 1/5\n",
            "131/131 [==============================] - 13s 68ms/step - loss: 0.4609 - accuracy: 0.8004 - val_loss: 0.2741 - val_accuracy: 0.9693\n",
            "Epoch 2/5\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 0.4324 - accuracy: 0.8118 - val_loss: 0.3107 - val_accuracy: 0.9650\n",
            "Epoch 3/5\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 0.4240 - accuracy: 0.8147 - val_loss: 0.2912 - val_accuracy: 0.9674\n",
            "Epoch 4/5\n",
            "131/131 [==============================] - 8s 61ms/step - loss: 0.4026 - accuracy: 0.8211 - val_loss: 0.2915 - val_accuracy: 0.9444\n",
            "Epoch 5/5\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 0.3792 - accuracy: 0.8304 - val_loss: 0.3311 - val_accuracy: 0.8926\n",
            "Epoch 1/5\n",
            "131/131 [==============================] - 13s 67ms/step - loss: 0.6046 - accuracy: 0.6879 - val_loss: 0.4041 - val_accuracy: 0.8825\n",
            "Epoch 2/5\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 0.5278 - accuracy: 0.7343 - val_loss: 0.3522 - val_accuracy: 0.8782\n",
            "Epoch 3/5\n",
            "131/131 [==============================] - 7s 54ms/step - loss: 0.4486 - accuracy: 0.7788 - val_loss: 0.4220 - val_accuracy: 0.8854\n",
            "Epoch 4/5\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 0.3992 - accuracy: 0.8043 - val_loss: 0.4552 - val_accuracy: 0.7952\n",
            "Epoch 5/5\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 0.3622 - accuracy: 0.8256 - val_loss: 0.3590 - val_accuracy: 0.8523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"InputTest.csv\")\n",
        "\n",
        "# Ajout manuel de 3 features (qui utilisent des Series et pas le dataframe entier)\n",
        "autocor = []\n",
        "idmax = []\n",
        "entropy = []\n",
        "for i in range(0,2488) :\n",
        "  autocor.append(test_data.iloc[i,2:2161].autocorr(216))\n",
        "  idmax.append((int)(re.search(r'\\d+',test_data.iloc[i,2:2161].idxmax())[0]))\n",
        "  entropy.append(sum([ p * math.log(p) / math.log(2.0) for p in test_data.iloc[i,2:2161]]))\n",
        "test_data['autocor'] = autocor\n",
        "test_data['idmax'] = idmax\n",
        "test_data['entropy'] = entropy\n",
        "\n",
        "# Ajout automatique de 6 features (qui utilisent le dataframe entier)\n",
        "test_data['mean'] = test_data.iloc[:, 2:2161].mean(axis=1)\n",
        "test_data['median'] = test_data.iloc[:, 2:2161].median(axis=1)\n",
        "test_data['std'] = test_data.iloc[:, 2:2161].std(axis=1)\n",
        "test_data['var'] = test_data.iloc[:, 2:2161].var(axis=1)\n",
        "test_data['skew'] = test_data.iloc[:, 2:2161].skew(axis=1)\n",
        "test_data['sem'] = test_data.iloc[:, 2:2161].sem(axis=1)\n",
        "\n",
        "test_input = test_data.drop([\"Index\"],axis=1)\n",
        "\n",
        "# Normaliser les données de test\n",
        "scaler = MinMaxScaler()\n",
        "test_input_scaled = scaler.fit_transform(test_input.iloc[:, 2:])\n",
        "\n",
        "test_data.fillna(0,inplace=True)"
      ],
      "metadata": {
        "id": "4v5hN5I9TV2M"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faire des prédictions avec les 5 modèles pour les données de test\n",
        "predictions = []\n",
        "for model in models:\n",
        "    prediction = model.predict(np.expand_dims(test_input_scaled, axis=2))\n",
        "    predictions.append(prediction.flatten())\n",
        "\n",
        "# Convertir les prédictions en un tableau de dimension (len(test_input), 5)\n",
        "class_preds = np.zeros((len(test_input), 5))\n",
        "for i in range(5):\n",
        "    class_preds[:, i] = (predictions[i] > 0.1).astype(int)\n",
        "\n",
        "# Enregistrer les prédictions dans un fichier CSV\n",
        "output_df = pd.DataFrame(class_preds, columns=[\"Washing Machine\", \"Dishwasher\", \"Tumble Dryer\", \"Microwave\", \"Kettle\"])\n",
        "output_df.insert(0, \"Index\", test_data[\"Index\"])\n",
        "output_df.to_csv(\"./new_features4.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "jOkl37BDhSu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5867a12b-83c8-427b-fe6d-01b736f67541"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 2s 22ms/step\n",
            "78/78 [==============================] - 2s 21ms/step\n",
            "78/78 [==============================] - 2s 24ms/step\n",
            "78/78 [==============================] - 3s 23ms/step\n",
            "78/78 [==============================] - 2s 21ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}